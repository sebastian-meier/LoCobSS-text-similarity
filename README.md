This repo is part of the LoCobSS research project. More details about the project and dependencies to other repos can be found [here](https://github.com/sebastian-meier/LoCobSS-documentation).

# LoCobSS-text-similarity
Endpoint that allows for a vptree-based similarity search in a set of vectors.

The setup is build for google cloud run & container registry.

## Build
```bash
gcloud builds submit --tag YOUR_TAG_NAME
```

## Deploy
We are running this on a 256MB Ram cloud function. The bigger the dataset the more memory is required. If your service is not responding, check the logs and make sure memory is not exceeding.
```bash
gcloud run deploy --image YOUR_TAG_NAME --platform managed
```

## Local build and deploy
You need to add *GOOGLE_APPLICATION_CREDENTIALS* to the **.env** file with the path to your credentials file.
Then simply install all requirements `pip install -r requirements.txt`, then first run `python setup.py` and then start gunicorn `gunicorn --bind :8080 --workers 1 --threads 4 --timeout 0 app:app`. If port *8080* is already occupied change accordingly.

## Configuration
Create a **.env** file based on **.env-sample**. On setup the system downloads ids and a numpy array file from Google Cloud Storage, therefore, names and buckets need to be provided.

## Getting the ids and numpy array file
The cloud function [LoCobSS-text-similarity-dataflow](https://github.com/sebastian-meier/LoCobSS-text-similarity-dataflow) is based on the [universal-sentence-encoder](https://tfhub.dev/google/universal-sentence-encoder/4) and converts a list of strings (taken from an mysql database) into embeddings (512-dimensional vectors). The resulting files are then stored in Google Cloud Storage.

## What about the ids?
In order to keep the data being send back and forth as small as possible each text has an ID (generated by mysql, but obviously could also be generated manually), similarity responses only return IDs, not texts. The texts need to be collected from the source independently.

## Endpoints
We build this for a demonstration. If you are building this for a highly frequented service, you might want to increase the workers and threads (will require more memory) in the last line of the **Dockerfile**.

### GET: URL/similar/<id>
Looks up the vector for the submitted id and returns the closest 10 matches (KNN).

### GET: URL/similar_random/<id>
Looks up the vector for the submitted id, collects the 100 best matches (KNN) and returns a random set of 10 matches (shuffle).

### GET: URL/update/similar/<id>
When the container is being build, the latest version of ids and embeddings are added to the container. Running queries on those files is fast. If the ids and embeddings are being updated, you can either rebuild container (most efficient if not too often) or you can run the update endpoint:
This will download the latest versions of the files and run similarity search on those (see URL/similar/<id>). But, as this is a stateless container, the latests versions will disappear upon the next query, which means it needs to be downloaded again. This makes this last endpoint rather slow (still acceptable).
